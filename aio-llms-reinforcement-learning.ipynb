{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Environment\n",
    "\n",
    "In this notebook, we will finetune LLama 3.2 1B with LoRA with GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 28 09:26:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:41:00.0 Off |                  Off |\n",
      "| 30%   33C    P8             30W /  450W |       0MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth vllm==0.7.3\n",
    "!pip install -U huggingface_hub\n",
    "!pip install -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msavoxism\u001b[0m (\u001b[33msavoxism-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250428_092644-oj5rilzp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO/runs/oj5rilzp' target=\"_blank\">track 1</a></strong> to <a href='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO' target=\"_blank\">https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO/runs/oj5rilzp' target=\"_blank\">https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO/runs/oj5rilzp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO/runs/oj5rilzp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4e8d26a9e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wb_token = \"79126da44d32381139323a9fc5fc6ba0e32b99c4\"\n",
    "wandb.login(key=wb_token)\n",
    "# wandb.init(project=\"Finetuning Qwen2.5 1.5B Math Instruct GRPO\", name=\"track 1\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "API_KEY = \"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\"\n",
    "login(token=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7496/1940001533.py:5: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
      "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-28 09:26:52 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from vllm import SamplingParams\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Qwen2.5-Math-1.5B-Instruct with actual GPU utilization = 78.56%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 23.53 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 256.\n",
      "Unsloth: vLLM's KV Cache can use up to 15.47 GB. Also swap space = 6 GB.\n",
      "INFO 04-28 09:27:10 config.py:549] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 04-28 09:27:10 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-28 09:27:11 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-28 09:27:12 model_runner.py:1110] Starting to load model unsloth/Qwen2.5-Math-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W428 09:27:12.874375495 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 09:27:12 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 04-28 09:27:13 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bce81b257f4290aafbc3f1e3cf30f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 09:27:14 model_runner.py:1115] Loading model weights took 2.8792 GB\n",
      "INFO 04-28 09:27:14 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-28 09:27:39 worker.py:267] Memory profiling takes 24.56 seconds\n",
      "INFO 04-28 09:27:39 worker.py:267] the current vLLM instance can use total_gpu_memory (23.53GiB) x gpu_memory_utilization (0.79) = 18.48GiB\n",
      "INFO 04-28 09:27:39 worker.py:267] model weights take 2.88GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 14.12GiB.\n",
      "INFO 04-28 09:27:40 executor_base.py:111] # cuda blocks: 33050, # CPU blocks: 14043\n",
      "INFO 04-28 09:27:40 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 258.20x\n",
      "INFO 04-28 09:27:49 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:31<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 09:28:20 model_runner.py:1562] Graph capturing finished in 31 secs, took 0.31 GiB\n",
      "INFO 04-28 09:28:20 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 66.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.4.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "lora_rank = 64\n",
    "SEED = 3407\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    load_in_4bit=False,# Turn off quantization to increase accuracy for reasoning\n",
    "    fast_inference=True, # optimize throughput\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=lora_rank,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ Total parameters:      1,617,573,376\n",
      "→ Trainable parameters:  73,859,072\n",
      "→ Frozen parameters:     1,543,714,304\n"
     ]
    }
   ],
   "source": [
    "# Counting parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"→ Total parameters:      {total_params:,}\")\n",
    "print(f\"→ Trainable parameters:  {trainable_params:,}\")\n",
    "print(f\"→ Frozen parameters:     {total_params-trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"5CD-AI/Vietnamese-meta-math-MetaMathQA-40K-gg-translated\"\n",
    "dataset = load_dataset(DATASET_PATH, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['response_vi', 'query_vi', 'response_en', 'type', 'query_en'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_vi': 'Để giải quyết vấn đề này, chúng ta cần xác định giá trị của x, biểu thị giá trị của mỗi cuốn sách mà Reggie mua. Hãy chia nhỏ thông tin đã cho: Số tiền mà bố Reggie đã cho anh ấy: $48 Số sách Reggie đã mua: 5 Số tiền Reggie đã để lại: $38 Chúng ta có thể thiết lập phương trình như sau: Số tiền mà bố Reggie đã cho anh ấy - (Số số sách Reggie đã mua * Chi phí mỗi cuốn sách) = Số tiền Reggie đã để lại $48 - (5 * x) = $38 Hãy đơn giản hóa và giải x: $48 - 5x = $38 Để tách x, chúng ta trừ $48 từ cả hai vế của phương trình : $48 - $48 - 5x = $38 - $48 -5x = -$10 Để giải x, chúng ta chia cả hai vế của phương trình cho -5: x = -$10 / -5 x = $2 Giá trị của x là $2. ####2 Đáp án là: 2',\n",
       " 'query_vi': 'Cha của Reggie đã cho anh ấy 48 đô la. Reggie đã mua 5 cuốn sách, mỗi cuốn có giá x. Reggie còn lại 38 tiền. Giá trị của biến x chưa biết là bao nhiêu?',\n",
       " 'response_en': \"To solve this problem, we need to determine the value of x, which represents the cost of each book that Reggie bought.\\nLet's break down the information given:\\nAmount of money Reggie's father gave him: $48\\nNumber of books Reggie bought: 5\\nAmount of money Reggie has left: $38\\nWe can set up the equation as follows:\\nAmount of money Reggie's father gave him - (Number of books Reggie bought * Cost per book) = Amount of money Reggie has left\\n$48 - (5 * x) = $38\\nLet's simplify and solve for x:\\n$48 - 5x = $38\\nTo isolate x, we subtract $48 from both sides of the equation:\\n$48 - $48 - 5x = $38 - $48\\n-5x = -$10\\nTo solve for x, we divide both sides of the equation by -5:\\nx = -$10 / -5\\nx = $2\\nThe value of x is $2.\\n#### 2\\nThe answer is: 2\",\n",
       " 'type': 'GSM_SV',\n",
       " 'query_en': \"Reggie's father gave him $48. Reggie bought 5 books, each of which cost x. Reggie has 38 money left. What is the value of unknown variable x?\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Configure LoRA\n",
    "+ Standardize data for the model to learning the reasoning trace and answers distinctively.\n",
    "+ Use `answer_pattern` to extract the answers.\n",
    "+ Signal the start/end of the reasoning chain with <thinking>...</thinking> and answer with <answer>...</answer>.\n",
    "+ Build `system_prompt` to guide the model to produce reasoning chain and then the answer.\n",
    "+ Change `train_dataset` to 2 fields `prompt` and `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_pattern = re.compile(\n",
    "#     r\"(đáp án là:|đáp án là :|câu trả lời là:|câu trả lời là :)\\s*(.*)\",\n",
    "#     re.IGNORECASE\n",
    "# )\n",
    "\n",
    "answer_pattern_en = re.compile(\n",
    "    r\"(?:the answer is:|answer:)\\s*(.*)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "formatted_dataset = []\n",
    "for item in dataset:\n",
    "    response = item['response_en'].strip().lower()\n",
    "    match = answer_pattern_en.search(response)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "        formatted_dataset.append({\n",
    "            \"question\": item['query_en'],\n",
    "            \"answer\": answer,\n",
    "        })\n",
    "\n",
    "reasoning_start = \"<thinking>\"\n",
    "reasoning_end   = \"</thinking>\"\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "    f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your thought process.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your final answer between {solution_start}{solution_end}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Cally and Danny washed their clothes. Cally has 10 white shirts, 5 colored shirts, 7 pairs of shorts, and 6 pairs of pants, while Danny has 6 white shirts, 8 colored shirts, 10 shorts, and 6 pairs of pants. How many clothes did they wash?',\n",
       " 'answer': '58'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9a2c1bff3545ed8e2e2be8ab10258c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_list(formatted_dataset[:8000])\n",
    "train_dataset = train_dataset.map(lambda x: {\n",
    "    \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": x['question']},\n",
    "    ],\n",
    "    \"answer\": x[\"answer\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'prompt'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"Reggie's father gave him $48. Reggie bought 5 books, each of which cost x. Reggie has 38 money left. What is the value of unknown variable x?\", 'answer': '2', 'prompt': [{'content': 'You are given a problem.\\nThink about the problem and provide your thought process.\\nPlace it between <thinking> and </thinking>.\\nThen, provide your final answer between <SOLUTION></SOLUTION>', 'role': 'system'}, {'content': \"Reggie's father gave him $48. Reggie bought 5 books, each of which cost x. Reggie has 38 money left. What is the value of unknown variable x?\", 'role': 'user'}]}\n",
      "****************************************************************************************************\n",
      "('<|im_start|>system\\n'\n",
      " 'You are given a problem.\\n'\n",
      " 'Think about the problem and provide your thought process.\\n'\n",
      " 'Place it between <thinking> and </thinking>.\\n'\n",
      " 'Then, provide your final answer between <SOLUTION></SOLUTION><|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " \"Reggie's father gave him $48. Reggie bought 5 books, each of which cost x. \"\n",
      " 'Reggie has 38 money left. What is the value of unknown variable '\n",
      " 'x?<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "sample = train_dataset[0]  \n",
    "\n",
    "print(sample)\n",
    "print(\"*\" * 100)\n",
    "text = tokenizer.apply_chat_template(\n",
    "    sample[\"prompt\"],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "pprint(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training LLM\n",
    "\n",
    "For reinforcment learning algorithm, evaluating the efficiency of the model is through the reward function. The reward functions evaluate the output based on: correct format reasoning and the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward for correct formatting\n",
    "match_format = re.compile(rf\"\"\"\n",
    "    ^\\s*                              # bất kỳ khoảng trắng đầu dòng\n",
    "    {re.escape(reasoning_start)}     # <thinking>\n",
    "    .*?                               # chain-of-thought (non-greedy)\n",
    "    {re.escape(reasoning_end)}        # </thinking>\n",
    "    .*?                               # có thể có text khác giữa\n",
    "    {re.escape(solution_start)}       # <SOLUTION>\n",
    "    (.+?)                             # nhóm 1: nội dung solution\n",
    "    {re.escape(solution_end)}         # </SOLUTION>\n",
    "    \\s*                               # optional trailing whitespace\n",
    "    $                                 # kết thúc chuỗi\n",
    "\"\"\", flags=re.DOTALL | re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0]['content']\n",
    "        if match_format.search(response) is not None:\n",
    "            score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0]['content']\n",
    "        # mỗi tag đúng một lần thì +0.5, thiếu hoặc lặp lại thì -1.0\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end) == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function `check_answer(` that p\n",
    "+ Parses the string between <answer> tags.\n",
    "+ Awards +3.0 points if the answer is exactly correct.\n",
    "+ Awards +1.5 points if it only differs by whitespace.\n",
    "+ Deducts 1.5 points if it’s completely wrong.\n",
    "\n",
    "Finally, we have a function `check_numbers()` that extracts numeric values from the response then compares them as floats.\n",
    "+ Awards +1.5 points for each correct number.\n",
    "+ Deducts 0.5 points for each incorrect number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward for correct answer\n",
    "match_solution = re.compile(\n",
    "    rf\"{re.escape(solution_start)}\\s*(.+?)\\s*{re.escape(solution_end)}\",\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    responses = [c[0]['content'] for c in completions]\n",
    "    extracted = [\n",
    "        m.group(1).strip() if (m := match_solution.search(r)) else None\n",
    "        for r in responses\n",
    "    ]\n",
    "    scores = []\n",
    "    for guess, true in zip(extracted, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0); continue\n",
    "        scores.append(\n",
    "            3.0 if guess == true\n",
    "            else 1.5 if guess.strip() == true.strip()\n",
    "            else -1.5\n",
    "        )\n",
    "    return scores\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    responses = [c[0]['content'] for c in completions]\n",
    "    extracted = [\n",
    "        m.group(1).strip() if (m := match_solution.search(r)) else None\n",
    "        for r in responses\n",
    "    ]\n",
    "    scores = []\n",
    "    for guess, true in zip(extracted, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0); continue\n",
    "        try:\n",
    "            t = float(true.replace(\",\", \"\"))\n",
    "            g = float(guess.replace(\",\", \"\"))\n",
    "            scores.append(1.5 if g == t else -0.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# match_numbers = re.compile(\n",
    "#     solution_start + r\".*?([\\d\\.\\,]{1,})\",\n",
    "#     flags=re.MULTILINE | re.DOTALL\n",
    "# )\n",
    "    \n",
    "# def check_answer(prompts, completions, answer, **kwargs):\n",
    "#     responses = [completion[0]['content'] for completion in completions]\n",
    "    \n",
    "#     extracted_responses = [\n",
    "#     m.group(1) if (m := match_numbers.search(r)) else None\n",
    "#     for r in responses\n",
    "#     ]\n",
    "    \n",
    "#     scores = []\n",
    "#     for guess, true_answer in zip(extracted_responses, answer):\n",
    "#         score = 0\n",
    "#         if guess is None:\n",
    "#             scores.append(0)\n",
    "#             continue\n",
    "#         if guess == true_answer:\n",
    "#             score += 3.0\n",
    "#         elif guess.strip() == true_answer.strip():\n",
    "#             score += 1.5\n",
    "#         else:\n",
    "#             score -= 1.5\n",
    "#         scores.append(score)\n",
    "#     return scores\n",
    "\n",
    "# def check_numbers(prompts, completions, answer, **kwargs):\n",
    "#     question = prompts[0][-1][\"content\"]\n",
    "#     responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "#     extracted_responses = [\n",
    "#         guess.group(1) if (guess := match_numbers.search(r)) is not None else None for r in responses\n",
    "#     ]\n",
    "\n",
    "#     count = getattr(check_numbers, 'counter', 0) + 1\n",
    "#     check_numbers.counter = count\n",
    "\n",
    "#     if count % 5 == 0:\n",
    "#         print('*'*20, f\"Question:{question}\", f\"\\nResponse:\\n{responses[0]}\",\n",
    "#               f\"\\nExtracted: {extracted_responses[0]}\", f\"\\nGT Answer: {answer[0]}\")\n",
    "\n",
    "#     scores = []\n",
    "#     for guess, true_answer in zip(extracted_responses, answer):\n",
    "#         if guess is None:\n",
    "#             scores.append(0)\n",
    "#             continue\n",
    "#         try:\n",
    "#             true_answer = float(true_answer.strip())\n",
    "#             # Remove commas like in 123,456\n",
    "#             guess = float(guess.strip().replace(\",\", \"\"))\n",
    "#             scores.append(1.5 if guess == true_answer else -0.5)\n",
    "#         except:\n",
    "#             scores.append(0)\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'prompt'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f332cc678dda4b4b9a7d011bf85b7d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2787d3c36645abb05efa39e63545aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = max(train_dataset.map(\n",
    "    lambda x: {\"tokens\": tokenizer.apply_chat_template(\n",
    "        x['prompt'], add_generation_prompt=True, tokenize=True)},\n",
    "    batched=True,\n",
    ").map(lambda x: {\"length\": len(x['tokens'])})['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 8,000 | Num Epochs = 1 | Total steps = 250\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 32\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 32 x 1) = 256\n",
      " \"-____-\"     Trainable parameters = 73,859,072/1,617,573,376 (4.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "max_prompt_length = max_len + 1\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    # Diagnostics\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"output_bz2\",\n",
    "    logging_steps=1,\n",
    "    logging_dir=\"output_bz2/logs\",  # thư mục chứa TensorBoard logs\n",
    "    run_name  = \"grpo-run1\",\n",
    "\n",
    "    # Optimization\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=5e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim='adamw_torch_fused',\n",
    "    max_grad_norm=0.1,\n",
    "\n",
    "    # Batch\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=32,\n",
    "\n",
    "    # Specific settings\n",
    "    num_generations=8,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_seq_length - max_prompt_length,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=-1,\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.state.global_step, \"/\", trainer.state.max_steps)\n",
    "print(\"Train time:\", trainer.state.log_history[-1][\"train_runtime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Run Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": train_dataset[idx][\"question\"]},\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "path_lora = \"grpo_saved_lora\"\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request= model.load_lora(path_lora),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(f\"Problem:\\n{train_dataset[idx][’question’]}\")\n",
    "print(f\"Response:\\n{output}\")\n",
    "print(\"GT Answer:\", train_dataset[idx][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could expand the method for more complicated problems or integrate with other diverse evaluating signals to further optimize the reasoning chians of LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_id = \"Savoxism/grpo-lora-vietnam-llm\"\n",
    "create_repo(repo_id, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder\n",
    "\n",
    "api = HfApi()\n",
    "# Đảm bảo repo đã được tạo ở bước 2\n",
    "upload_folder(\n",
    "    folder_path=\"grpo_saved_lora\",  # thư mục local\n",
    "    repo_id=repo_id,                # ví dụ \"your_username/grpo-lora-vietnam-llm\"\n",
    "    repo_type=\"model\"               # loại repository\n",
    ")  # upload toàn bộ folder lên Hub :contentReference[oaicite:8]{index=8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(\"your_username/grpo-lora-vietnam-llm\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
