{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Environment\n",
    "\n",
    "In this notebook, we will finetune LLama 3.2 1B with LoRA with GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Apr 28 09:26:38 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.133.07             Driver Version: 570.133.07     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  |   00000000:41:00.0 Off |                  Off |\n",
      "| 30%   33C    P8             30W /  450W |       0MiB /  24564MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth vllm==0.7.3\n",
    "!pip install -U huggingface_hub\n",
    "!pip install -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msavoxism\u001b[0m (\u001b[33msavoxism-hanoi-university-of-science-and-technology\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.10"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/wandb/run-20250428_092644-oj5rilzp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO/runs/oj5rilzp' target=\"_blank\">track 1</a></strong> to <a href='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO' target=\"_blank\">https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO/runs/oj5rilzp' target=\"_blank\">https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO/runs/oj5rilzp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/savoxism-hanoi-university-of-science-and-technology/Finetuning%20Qwen2.5%201.5B%20Math%20Instruct%20GRPO/runs/oj5rilzp?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7f4e8d26a9e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "wb_token = \"79126da44d32381139323a9fc5fc6ba0e32b99c4\"\n",
    "wandb.login(key=wb_token)\n",
    "# wandb.init(project=\"Finetuning Qwen2.5 1.5B Math Instruct GRPO\", name=\"track 1\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "API_KEY = \"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\"\n",
    "login(token=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7496/1940001533.py:5: UserWarning: WARNING: Unsloth should be imported before transformers to ensure all optimizations are applied. Your code may run slower or encounter memory issues without these optimizations.\n",
      "\n",
      "Please restructure your imports with 'import unsloth' at the top of your file.\n",
      "  from unsloth import FastLanguageModel\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "Unsloth: Failed to patch SmolVLMForConditionalGeneration forward function.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "INFO 04-28 09:26:52 __init__.py:207] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "from vllm import SamplingParams\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.4.1: Fast Qwen2 patching. Transformers: 4.51.3. vLLM: 0.7.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 4090. Num GPUs = 1. Max memory: 23.527 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.5.1+cu124. CUDA: 8.9. CUDA Toolkit: 12.4. Triton: 3.1.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.28.post3. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: vLLM loading unsloth/Qwen2.5-Math-1.5B-Instruct with actual GPU utilization = 78.56%\n",
      "Unsloth: Your GPU has CUDA compute capability 8.9 with VRAM = 23.53 GB.\n",
      "Unsloth: Using conservativeness = 1.0. Chunked prefill tokens = 2048. Num Sequences = 256.\n",
      "Unsloth: vLLM's KV Cache can use up to 15.47 GB. Also swap space = 6 GB.\n",
      "INFO 04-28 09:27:10 config.py:549] This model supports multiple tasks: {'generate', 'score', 'classify', 'reward', 'embed'}. Defaulting to 'generate'.\n",
      "INFO 04-28 09:27:10 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.3) with config: model='unsloth/Qwen2.5-Math-1.5B-Instruct', speculative_config=None, tokenizer='unsloth/Qwen2.5-Math-1.5B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda:0, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=unsloth/Qwen2.5-Math-1.5B-Instruct, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=False, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":0,\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 04-28 09:27:11 cuda.py:229] Using Flash Attention backend.\n",
      "INFO 04-28 09:27:12 model_runner.py:1110] Starting to load model unsloth/Qwen2.5-Math-1.5B-Instruct...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W428 09:27:12.874375495 CUDAAllocatorConfig.h:28] Warning: expandable_segments not supported on this platform (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 09:27:12 weight_utils.py:254] Using model weights format ['*.safetensors']\n",
      "INFO 04-28 09:27:13 weight_utils.py:304] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98bce81b257f4290aafbc3f1e3cf30f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 09:27:14 model_runner.py:1115] Loading model weights took 2.8792 GB\n",
      "INFO 04-28 09:27:14 punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 04-28 09:27:39 worker.py:267] Memory profiling takes 24.56 seconds\n",
      "INFO 04-28 09:27:39 worker.py:267] the current vLLM instance can use total_gpu_memory (23.53GiB) x gpu_memory_utilization (0.79) = 18.48GiB\n",
      "INFO 04-28 09:27:39 worker.py:267] model weights take 2.88GiB; non_torch_memory takes 0.08GiB; PyTorch activation peak memory takes 1.41GiB; the rest of the memory reserved for KV Cache is 14.12GiB.\n",
      "INFO 04-28 09:27:40 executor_base.py:111] # cuda blocks: 33050, # CPU blocks: 14043\n",
      "INFO 04-28 09:27:40 executor_base.py:116] Maximum concurrency for 2048 tokens per request: 258.20x\n",
      "INFO 04-28 09:27:49 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:31<00:00,  1.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-28 09:28:20 model_runner.py:1562] Graph capturing finished in 31 secs, took 0.31 GiB\n",
      "INFO 04-28 09:28:20 llm_engine.py:436] init engine (profile, create kv cache, warmup model) took 66.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Unsloth 2025.4.1 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 2048\n",
    "lora_rank = 64\n",
    "SEED = 3407\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    load_in_4bit=False,# Turn off quantization to increase accuracy for reasoning\n",
    "    fast_inference=True, # optimize throughput\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=lora_rank,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚Üí Total parameters:      1,617,573,376\n",
      "‚Üí Trainable parameters:  73,859,072\n",
      "‚Üí Frozen parameters:     1,543,714,304\n"
     ]
    }
   ],
   "source": [
    "# Counting parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"‚Üí Total parameters:      {total_params:,}\")\n",
    "print(f\"‚Üí Trainable parameters:  {trainable_params:,}\")\n",
    "print(f\"‚Üí Frozen parameters:     {total_params-trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"5CD-AI/Vietnamese-meta-math-MetaMathQA-40K-gg-translated\"\n",
    "dataset = load_dataset(DATASET_PATH, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['response_vi', 'query_vi', 'response_en', 'type', 'query_en'],\n",
       "    num_rows: 40000\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'response_vi': 'ƒê·ªÉ gi·∫£i quy·∫øt v·∫•n ƒë·ªÅ n√†y, ch√∫ng ta c·∫ßn x√°c ƒë·ªãnh gi√° tr·ªã c·ªßa x, bi·ªÉu th·ªã gi√° tr·ªã c·ªßa m·ªói cu·ªën s√°ch m√† Reggie mua. H√£y chia nh·ªè th√¥ng tin ƒë√£ cho: S·ªë ti·ªÅn m√† b·ªë Reggie ƒë√£ cho anh ·∫•y: $48 S·ªë s√°ch Reggie ƒë√£ mua: 5 S·ªë ti·ªÅn Reggie ƒë√£ ƒë·ªÉ l·∫°i: $38 Ch√∫ng ta c√≥ th·ªÉ thi·∫øt l·∫≠p ph∆∞∆°ng tr√¨nh nh∆∞ sau: S·ªë ti·ªÅn m√† b·ªë Reggie ƒë√£ cho anh ·∫•y - (S·ªë s·ªë s√°ch Reggie ƒë√£ mua * Chi ph√≠ m·ªói cu·ªën s√°ch) = S·ªë ti·ªÅn Reggie ƒë√£ ƒë·ªÉ l·∫°i $48 - (5 * x) = $38 H√£y ƒë∆°n gi·∫£n h√≥a v√† gi·∫£i x: $48 - 5x = $38 ƒê·ªÉ t√°ch x, ch√∫ng ta tr·ª´ $48 t·ª´ c·∫£ hai v·∫ø c·ªßa ph∆∞∆°ng tr√¨nh : $48 - $48 - 5x = $38 - $48 -5x = -$10 ƒê·ªÉ gi·∫£i x, ch√∫ng ta chia c·∫£ hai v·∫ø c·ªßa ph∆∞∆°ng tr√¨nh cho -5: x = -$10 / -5 x = $2 Gi√° tr·ªã c·ªßa x l√† $2. ####2 ƒê√°p √°n l√†: 2',\n",
       " 'query_vi': 'Cha c·ªßa Reggie ƒë√£ cho anh ·∫•y 48 ƒë√¥ la. Reggie ƒë√£ mua 5 cu·ªën s√°ch, m·ªói cu·ªën c√≥ gi√° x. Reggie c√≤n l·∫°i 38 ti·ªÅn. Gi√° tr·ªã c·ªßa bi·∫øn x ch∆∞a bi·∫øt l√† bao nhi√™u?',\n",
       " 'response_en': \"To solve this problem, we need to determine the value of x, which represents the cost of each book that Reggie bought.\\nLet's break down the information given:\\nAmount of money Reggie's father gave him: $48\\nNumber of books Reggie bought: 5\\nAmount of money Reggie has left: $38\\nWe can set up the equation as follows:\\nAmount of money Reggie's father gave him - (Number of books Reggie bought * Cost per book) = Amount of money Reggie has left\\n$48 - (5 * x) = $38\\nLet's simplify and solve for x:\\n$48 - 5x = $38\\nTo isolate x, we subtract $48 from both sides of the equation:\\n$48 - $48 - 5x = $38 - $48\\n-5x = -$10\\nTo solve for x, we divide both sides of the equation by -5:\\nx = -$10 / -5\\nx = $2\\nThe value of x is $2.\\n#### 2\\nThe answer is: 2\",\n",
       " 'type': 'GSM_SV',\n",
       " 'query_en': \"Reggie's father gave him $48. Reggie bought 5 books, each of which cost x. Reggie has 38 money left. What is the value of unknown variable x?\"}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Configure LoRA\n",
    "+ Standardize data for the model to learning the reasoning trace and answers distinctively.\n",
    "+ Use `answer_pattern` to extract the answers.\n",
    "+ Signal the start/end of the reasoning chain with <thinking>...</thinking> and answer with <answer>...</answer>.\n",
    "+ Build `system_prompt` to guide the model to produce reasoning chain and then the answer.\n",
    "+ Change `train_dataset` to 2 fields `prompt` and `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_pattern = re.compile(\n",
    "#     r\"(ƒë√°p √°n l√†:|ƒë√°p √°n l√† :|c√¢u tr·∫£ l·ªùi l√†:|c√¢u tr·∫£ l·ªùi l√† :)\\s*(.*)\",\n",
    "#     re.IGNORECASE\n",
    "# )\n",
    "\n",
    "answer_pattern_en = re.compile(\n",
    "    r\"(?:the answer is:|answer:)\\s*(.*)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "formatted_dataset = []\n",
    "for item in dataset:\n",
    "    response = item['response_en'].strip().lower()\n",
    "    match = answer_pattern_en.search(response)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "        formatted_dataset.append({\n",
    "            \"question\": item['query_en'],\n",
    "            \"answer\": answer,\n",
    "        })\n",
    "\n",
    "reasoning_start = \"<thinking>\"\n",
    "reasoning_end   = \"</thinking>\"\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "    f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your thought process.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your final answer between {solution_start}{solution_end}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Cally and Danny washed their clothes. Cally has 10 white shirts, 5 colored shirts, 7 pairs of shorts, and 6 pairs of pants, while Danny has 6 white shirts, 8 colored shirts, 10 shorts, and 6 pairs of pants. How many clothes did they wash?',\n",
       " 'answer': '58'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c9a2c1bff3545ed8e2e2be8ab10258c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = Dataset.from_list(formatted_dataset[:8000])\n",
    "train_dataset = train_dataset.map(lambda x: {\n",
    "    \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": x['question']},\n",
    "    ],\n",
    "    \"answer\": x[\"answer\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'prompt'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'question': \"Reggie's father gave him $48. Reggie bought 5 books, each of which cost x. Reggie has 38 money left. What is the value of unknown variable x?\", 'answer': '2', 'prompt': [{'content': 'You are given a problem.\\nThink about the problem and provide your thought process.\\nPlace it between <thinking> and </thinking>.\\nThen, provide your final answer between <SOLUTION></SOLUTION>', 'role': 'system'}, {'content': \"Reggie's father gave him $48. Reggie bought 5 books, each of which cost x. Reggie has 38 money left. What is the value of unknown variable x?\", 'role': 'user'}]}\n",
      "****************************************************************************************************\n",
      "('<|im_start|>system\\n'\n",
      " 'You are given a problem.\\n'\n",
      " 'Think about the problem and provide your thought process.\\n'\n",
      " 'Place it between <thinking> and </thinking>.\\n'\n",
      " 'Then, provide your final answer between <SOLUTION></SOLUTION><|im_end|>\\n'\n",
      " '<|im_start|>user\\n'\n",
      " \"Reggie's father gave him $48. Reggie bought 5 books, each of which cost x. \"\n",
      " 'Reggie has 38 money left. What is the value of unknown variable '\n",
      " 'x?<|im_end|>\\n'\n",
      " '<|im_start|>assistant\\n')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "sample = train_dataset[0]  \n",
    "\n",
    "print(sample)\n",
    "print(\"*\" * 100)\n",
    "text = tokenizer.apply_chat_template(\n",
    "    sample[\"prompt\"],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "pprint(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training LLM\n",
    "\n",
    "For reinforcment learning algorithm, evaluating the efficiency of the model is through the reward function. The reward functions evaluate the output based on: correct format reasoning and the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward for correct formatting\n",
    "match_format = re.compile(rf\"\"\"\n",
    "    ^\\s*                              # b·∫•t k·ª≥ kho·∫£ng tr·∫Øng ƒë·∫ßu d√≤ng\n",
    "    {re.escape(reasoning_start)}     # <thinking>\n",
    "    .*?                               # chain-of-thought (non-greedy)\n",
    "    {re.escape(reasoning_end)}        # </thinking>\n",
    "    .*?                               # c√≥ th·ªÉ c√≥ text kh√°c gi·ªØa\n",
    "    {re.escape(solution_start)}       # <SOLUTION>\n",
    "    (.+?)                             # nh√≥m 1: n·ªôi dung solution\n",
    "    {re.escape(solution_end)}         # </SOLUTION>\n",
    "    \\s*                               # optional trailing whitespace\n",
    "    $                                 # k·∫øt th√∫c chu·ªói\n",
    "\"\"\", flags=re.DOTALL | re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0]['content']\n",
    "        if match_format.search(response) is not None:\n",
    "            score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0]['content']\n",
    "        # m·ªói tag ƒë√∫ng m·ªôt l·∫ßn th√¨ +0.5, thi·∫øu ho·∫∑c l·∫∑p l·∫°i th√¨ -1.0\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end) == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function `check_answer(` that p\n",
    "+ Parses the string between <answer> tags.\n",
    "+ Awards +3.0 points if the answer is exactly correct.\n",
    "+ Awards +1.5 points if it only differs by whitespace.\n",
    "+ Deducts 1.5 points if it‚Äôs completely wrong.\n",
    "\n",
    "Finally, we have a function `check_numbers()` that extracts numeric values from the response then compares them as floats.\n",
    "+ Awards +1.5 points for each correct number.\n",
    "+ Deducts 0.5 points for each incorrect number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward for correct answer\n",
    "match_solution = re.compile(\n",
    "    rf\"{re.escape(solution_start)}\\s*(.+?)\\s*{re.escape(solution_end)}\",\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    responses = [c[0]['content'] for c in completions]\n",
    "    extracted = [\n",
    "        m.group(1).strip() if (m := match_solution.search(r)) else None\n",
    "        for r in responses\n",
    "    ]\n",
    "    scores = []\n",
    "    for guess, true in zip(extracted, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0); continue\n",
    "        scores.append(\n",
    "            3.0 if guess == true\n",
    "            else 1.5 if guess.strip() == true.strip()\n",
    "            else -1.5\n",
    "        )\n",
    "    return scores\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    responses = [c[0]['content'] for c in completions]\n",
    "    extracted = [\n",
    "        m.group(1).strip() if (m := match_solution.search(r)) else None\n",
    "        for r in responses\n",
    "    ]\n",
    "    scores = []\n",
    "    for guess, true in zip(extracted, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0); continue\n",
    "        try:\n",
    "            t = float(true.replace(\",\", \"\"))\n",
    "            g = float(guess.replace(\",\", \"\"))\n",
    "            scores.append(1.5 if g == t else -0.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "    return scores\n",
    "\n",
    "\n",
    "# match_numbers = re.compile(\n",
    "#     solution_start + r\".*?([\\d\\.\\,]{1,})\",\n",
    "#     flags=re.MULTILINE | re.DOTALL\n",
    "# )\n",
    "    \n",
    "# def check_answer(prompts, completions, answer, **kwargs):\n",
    "#     responses = [completion[0]['content'] for completion in completions]\n",
    "    \n",
    "#     extracted_responses = [\n",
    "#     m.group(1) if (m := match_numbers.search(r)) else None\n",
    "#     for r in responses\n",
    "#     ]\n",
    "    \n",
    "#     scores = []\n",
    "#     for guess, true_answer in zip(extracted_responses, answer):\n",
    "#         score = 0\n",
    "#         if guess is None:\n",
    "#             scores.append(0)\n",
    "#             continue\n",
    "#         if guess == true_answer:\n",
    "#             score += 3.0\n",
    "#         elif guess.strip() == true_answer.strip():\n",
    "#             score += 1.5\n",
    "#         else:\n",
    "#             score -= 1.5\n",
    "#         scores.append(score)\n",
    "#     return scores\n",
    "\n",
    "# def check_numbers(prompts, completions, answer, **kwargs):\n",
    "#     question = prompts[0][-1][\"content\"]\n",
    "#     responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "#     extracted_responses = [\n",
    "#         guess.group(1) if (guess := match_numbers.search(r)) is not None else None for r in responses\n",
    "#     ]\n",
    "\n",
    "#     count = getattr(check_numbers, 'counter', 0) + 1\n",
    "#     check_numbers.counter = count\n",
    "\n",
    "#     if count % 5 == 0:\n",
    "#         print('*'*20, f\"Question:{question}\", f\"\\nResponse:\\n{responses[0]}\",\n",
    "#               f\"\\nExtracted: {extracted_responses[0]}\", f\"\\nGT Answer: {answer[0]}\")\n",
    "\n",
    "#     scores = []\n",
    "#     for guess, true_answer in zip(extracted_responses, answer):\n",
    "#         if guess is None:\n",
    "#             scores.append(0)\n",
    "#             continue\n",
    "#         try:\n",
    "#             true_answer = float(true_answer.strip())\n",
    "#             # Remove commas like in 123,456\n",
    "#             guess = float(guess.strip().replace(\",\", \"\"))\n",
    "#             scores.append(1.5 if guess == true_answer else -0.5)\n",
    "#         except:\n",
    "#             scores.append(0)\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Save Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['question', 'answer', 'prompt'],\n",
       "    num_rows: 8000\n",
       "})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f332cc678dda4b4b9a7d011bf85b7d69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af2787d3c36645abb05efa39e63545aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_len = max(train_dataset.map(\n",
    "    lambda x: {\"tokens\": tokenizer.apply_chat_template(\n",
    "        x['prompt'], add_generation_prompt=True, tokenize=True)},\n",
    "    batched=True,\n",
    ").map(lambda x: {\"length\": len(x['tokens'])})['length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "995"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 8,000 | Num Epochs = 1 | Total steps = 250\n",
      "O^O/ \\_/ \\    Batch size per device = 8 | Gradient accumulation steps = 32\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (8 x 32 x 1) = 256\n",
      " \"-____-\"     Trainable parameters = 73,859,072/1,617,573,376 (4.57% trained)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n"
     ]
    }
   ],
   "source": [
    "max_prompt_length = max_len + 1\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    # Diagnostics\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"output_bz2\",\n",
    "    logging_steps=1,\n",
    "    logging_dir=\"output_bz2/logs\",  # th∆∞ m·ª•c ch·ª©a TensorBoard logs\n",
    "    run_name  = \"grpo-run1\",\n",
    "\n",
    "    # Optimization\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=5e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim='adamw_torch_fused',\n",
    "    max_grad_norm=0.1,\n",
    "\n",
    "    # Batch\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=32,\n",
    "\n",
    "    # Specific settings\n",
    "    num_generations=8,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_seq_length - max_prompt_length,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=-1,\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainer.state.global_step, \"/\", trainer.state.max_steps)\n",
    "print(\"Train time:\", trainer.state.log_history[-1][\"train_runtime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Run Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": train_dataset[idx][\"question\"]},\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "path_lora = \"grpo_saved_lora\"\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request= model.load_lora(path_lora),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(f\"Problem:\\n{train_dataset[idx][‚Äôquestion‚Äô]}\")\n",
    "print(f\"Response:\\n{output}\")\n",
    "print(\"GT Answer:\", train_dataset[idx][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could expand the method for more complicated problems or integrate with other diverse evaluating signals to further optimize the reasoning chians of LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_id = \"Savoxism/grpo-lora-vietnam-llm\"\n",
    "create_repo(repo_id, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder\n",
    "\n",
    "api = HfApi()\n",
    "# ƒê·∫£m b·∫£o repo ƒë√£ ƒë∆∞·ª£c t·∫°o ·ªü b∆∞·ªõc 2\n",
    "upload_folder(\n",
    "    folder_path=\"grpo_saved_lora\",  # th∆∞ m·ª•c local\n",
    "    repo_id=repo_id,                # v√≠ d·ª• \"your_username/grpo-lora-vietnam-llm\"\n",
    "    repo_type=\"model\"               # lo·∫°i repository\n",
    ")  # upload to√†n b·ªô folder l√™n Hub :contentReference[oaicite:8]{index=8}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(\"your_username/grpo-lora-vietnam-llm\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
