{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Setup Environment\n",
    "\n",
    "In this notebook, we will finetune LLama 3.2 1B with LoRA with GRPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install unsloth vllm==0.7.3\n",
    "!pip install -U huggingface_hub\n",
    "!pip install -U wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wb_token = \"79126da44d32381139323a9fc5fc6ba0e32b99c4\"\n",
    "wandb.login(key=wb_token)\n",
    "# wandb.init(project=\"Finetuning Qwen2.5 1.5B Math Instruct GRPO\", name=\"track 1\", reinit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "API_KEY = \"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\"\n",
    "login(token=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "from vllm import SamplingParams\n",
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset, Dataset\n",
    "from trl import GRPOConfig, GRPOTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Load Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "lora_rank = 32\n",
    "SEED = 42\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-Math-1.5B-Instruct\"\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=MODEL_NAME,\n",
    "    load_in_4bit=False,# Turn off quantization to increase accuracy for reasoning\n",
    "    fast_inference=True, # optimize throughput\n",
    "    max_lora_rank=lora_rank,\n",
    "    gpu_memory_utilization=0.8,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=lora_rank,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
    "    ],\n",
    "    lora_alpha=64,\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=SEED,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"→ Total parameters:      {total_params:,}\")\n",
    "print(f\"→ Trainable parameters:  {trainable_params:,}\")\n",
    "print(f\"→ Frozen parameters:     {total_params-trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"5CD-AI/Vietnamese-meta-math-MetaMathQA-40K-gg-translated\"\n",
    "dataset = load_dataset(DATASET_PATH, split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Configure LoRA\n",
    "+ Standardize data for the model to learning the reasoning trace and answers distinctively.\n",
    "+ Use `answer_pattern` to extract the answers.\n",
    "+ Signal the start/end of the reasoning chain with <thinking>...</thinking> and answer with <answer>...</answer>.\n",
    "+ Build `system_prompt` to guide the model to produce reasoning chain and then the answer.\n",
    "+ Change `train_dataset` to 2 fields `prompt` and `answer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer_pattern = re.compile(\n",
    "#     r\"(đáp án là:|đáp án là :|câu trả lời là:|câu trả lời là :)\\s*(.*)\",\n",
    "#     re.IGNORECASE\n",
    "# )\n",
    "\n",
    "answer_pattern_en = re.compile(\n",
    "    r\"(?:the answer is:|answer:)\\s*(.*)\",\n",
    "    re.IGNORECASE\n",
    ")\n",
    "\n",
    "formatted_dataset = []\n",
    "# Fix this loop correspondingly\n",
    "for item in dataset:\n",
    "    response = item['response_en'].strip().lower()\n",
    "    match = answer_pattern_en.search(response)\n",
    "    if match:\n",
    "        answer = match.group(1).strip()\n",
    "        formatted_dataset.append({\n",
    "            \"question\": item['query_en'],\n",
    "            \"answer\": answer,\n",
    "        })\n",
    "\n",
    "reasoning_start = \"<thinking>\"\n",
    "reasoning_end   = \"</thinking>\"\n",
    "solution_start  = \"<SOLUTION>\"\n",
    "solution_end    = \"</SOLUTION>\"\n",
    "\n",
    "system_prompt = \\\n",
    "    f\"\"\"You are given a problem.\n",
    "Think about the problem and provide your thought process.\n",
    "Place it between {reasoning_start} and {reasoning_end}.\n",
    "Then, provide your final answer between {solution_start}{solution_end}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(formatted_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "formatted_dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_list(formatted_dataset[:8000])\n",
    "train_dataset = train_dataset.map(lambda x: {\n",
    "    \"prompt\": [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": x['question']},\n",
    "    ],\n",
    "    \"answer\": x[\"answer\"],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "sample = train_dataset[0]  \n",
    "\n",
    "print(sample)\n",
    "print(\"*\" * 100)\n",
    "text = tokenizer.apply_chat_template(\n",
    "    sample[\"prompt\"],\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False\n",
    ")\n",
    "\n",
    "pprint(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Training LLM\n",
    "\n",
    "For reinforcment learning algorithm, evaluating the efficiency of the model is through the reward function. The reward functions evaluate the output based on: correct format reasoning and the correct answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward for correct formatting\n",
    "match_format = re.compile(rf\"\"\"\n",
    "    ^\\s*                              # bất kỳ khoảng trắng đầu dòng\n",
    "    {re.escape(reasoning_start)}     # <thinking>\n",
    "    .*?                               # chain-of-thought (non-greedy)\n",
    "    {re.escape(reasoning_end)}        # </thinking>\n",
    "    .*?                               # có thể có text khác giữa\n",
    "    {re.escape(solution_start)}       # <SOLUTION>\n",
    "    (.+?)                             # nhóm 1: nội dung solution\n",
    "    {re.escape(solution_end)}         # </SOLUTION>\n",
    "    \\s*                               # optional trailing whitespace\n",
    "    $                                 # kết thúc chuỗi\n",
    "\"\"\", flags=re.DOTALL | re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "def match_format_exactly(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0]['content']\n",
    "        if match_format.search(response) is not None:\n",
    "            score += 3.0\n",
    "        scores.append(score)\n",
    "    return scores\n",
    "\n",
    "def match_format_approximately(completions, **kwargs):\n",
    "    scores = []\n",
    "    for completion in completions:\n",
    "        score = 0\n",
    "        response = completion[0]['content']\n",
    "        # mỗi tag đúng một lần thì +0.5, thiếu hoặc lặp lại thì -1.0\n",
    "        score += 0.5 if response.count(reasoning_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(reasoning_end) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_start) == 1 else -1.0\n",
    "        score += 0.5 if response.count(solution_end) == 1 else -1.0\n",
    "        scores.append(score)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a function `check_answer(` that p\n",
    "+ Parses the string between <answer> tags.\n",
    "+ Awards +3.0 points if the answer is exactly correct.\n",
    "+ Awards +1.5 points if it only differs by whitespace.\n",
    "+ Deducts 1.5 points if it’s completely wrong.\n",
    "\n",
    "Finally, we have a function `check_numbers()` that extracts numeric values from the response then compares them as floats.\n",
    "+ Awards +1.5 points for each correct number.\n",
    "+ Deducts 0.5 points for each incorrect number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match_numbers = re.compile(\n",
    "#     solution_start + r\".*?([\\d\\.\\,]{1,})\",\n",
    "#     flags=re.MULTILINE | re.DOTALL\n",
    "# )\n",
    "    \n",
    "# def check_answer(prompts, completions, answer, **kwargs):\n",
    "#     responses = [completion[0]['content'] for completion in completions]\n",
    "    \n",
    "#     extracted_responses = [\n",
    "#     m.group(1) if (m := match_numbers.search(r)) else None\n",
    "#     for r in responses\n",
    "#     ]\n",
    "    \n",
    "#     scores = []\n",
    "#     for guess, true_answer in zip(extracted_responses, answer):\n",
    "#         score = 0\n",
    "#         if guess is None:\n",
    "#             scores.append(0)\n",
    "#             continue\n",
    "#         if guess == true_answer:\n",
    "#             score += 3.0\n",
    "#         elif guess.strip() == true_answer.strip():\n",
    "#             score += 1.5\n",
    "#         else:\n",
    "#             score -= 1.5\n",
    "#         scores.append(score)\n",
    "#     return scores\n",
    "\n",
    "# def check_numbers(prompts, completions, answer, **kwargs):\n",
    "#     question = prompts[0][-1][\"content\"]\n",
    "#     responses = [completion[0][\"content\"] for completion in completions]\n",
    "\n",
    "#     extracted_responses = [\n",
    "#         guess.group(1) if (guess := match_numbers.search(r)) is not None else None for r in responses\n",
    "#     ]\n",
    "\n",
    "#     count = getattr(check_numbers, 'counter', 0) + 1\n",
    "#     check_numbers.counter = count\n",
    "\n",
    "#     if count % 5 == 0:\n",
    "#         print('*'*20, f\"Question:{question}\", f\"\\nResponse:\\n{responses[0]}\",\n",
    "#               f\"\\nExtracted: {extracted_responses[0]}\", f\"\\nGT Answer: {answer[0]}\")\n",
    "\n",
    "#     scores = []\n",
    "#     for guess, true_answer in zip(extracted_responses, answer):\n",
    "#         if guess is None:\n",
    "#             scores.append(0)\n",
    "#             continue\n",
    "#         try:\n",
    "#             true_answer = float(true_answer.strip())\n",
    "#             # Remove commas like in 123,456\n",
    "#             guess = float(guess.strip().replace(\",\", \"\"))\n",
    "#             scores.append(1.5 if guess == true_answer else -0.5)\n",
    "#         except:\n",
    "#             scores.append(0)\n",
    "#     return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward for correct answer\n",
    "match_solution = re.compile(\n",
    "    rf\"{re.escape(solution_start)}\\s*(.+?)\\s*{re.escape(solution_end)}\",\n",
    "    flags=re.DOTALL\n",
    ")\n",
    "\n",
    "def check_answer(prompts, completions, answer, **kwargs):\n",
    "    responses = [c[0]['content'] for c in completions]\n",
    "    extracted = [\n",
    "        m.group(1).strip() if (m := match_solution.search(r)) else None\n",
    "        for r in responses\n",
    "    ]\n",
    "    scores = []\n",
    "    for guess, true in zip(extracted, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0); continue\n",
    "        scores.append(\n",
    "            3.0 if guess == true\n",
    "            else 1.5 if guess.strip() == true.strip()\n",
    "            else -1.5\n",
    "        )\n",
    "    return scores\n",
    "\n",
    "def check_numbers(prompts, completions, answer, **kwargs):\n",
    "    responses = [c[0]['content'] for c in completions]\n",
    "    extracted = [\n",
    "        m.group(1).strip() if (m := match_solution.search(r)) else None\n",
    "        for r in responses\n",
    "    ]\n",
    "    scores = []\n",
    "    for guess, true in zip(extracted, answer):\n",
    "        if guess is None:\n",
    "            scores.append(0); continue\n",
    "        try:\n",
    "            t = float(true.replace(\",\", \"\"))\n",
    "            g = float(guess.replace(\",\", \"\"))\n",
    "            scores.append(1.5 if g == t else -0.5)\n",
    "        except:\n",
    "            scores.append(0)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Finetuning & Saving Checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = max(train_dataset.map(\n",
    "    lambda x: {\"tokens\": tokenizer.apply_chat_template(\n",
    "        x['prompt'], add_generation_prompt=True, tokenize=True)},\n",
    "    batched=True,\n",
    ").map(lambda x: {\"length\": len(x['tokens'])})['length'])\n",
    "\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_prompt_length = max_len + 1\n",
    "\n",
    "training_args = GRPOConfig(\n",
    "    # Diagnostics\n",
    "    report_to=\"wandb\",\n",
    "    output_dir=\"output_bz2\",\n",
    "    logging_steps=1,\n",
    "    logging_dir=\"output_bz2/logs\",  # thư mục chứa TensorBoard logs\n",
    "    run_name  = \"grpo-run1\",\n",
    "\n",
    "    # Optimization\n",
    "    learning_rate=5e-6,\n",
    "    weight_decay=5e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    optim='adamw_torch_fused',\n",
    "    max_grad_norm=0.1,\n",
    "\n",
    "    # Batch\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=32,\n",
    "\n",
    "    # Specific settings\n",
    "    num_generations=8,\n",
    "    max_prompt_length=max_prompt_length,\n",
    "    max_completion_length=max_seq_length - max_prompt_length,\n",
    "    num_train_epochs=1,\n",
    "    max_steps=-1,\n",
    "    save_steps=50,\n",
    ")\n",
    "\n",
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        match_format_exactly,\n",
    "        match_format_approximately,\n",
    "        check_answer,\n",
    "        check_numbers,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the training time\n",
    "print(trainer.state.global_step, \"/\", trainer.state.max_steps)\n",
    "print(\"Train time:\", trainer.state.log_history[-1][\"train_runtime\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Run Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_lora(\"grpo_saved_lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 2\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": train_dataset[idx][\"question\"]},\n",
    "]\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    max_tokens=1024,\n",
    ")\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=False,\n",
    ")\n",
    "\n",
    "path_lora = \"grpo_saved_lora\"\n",
    "output = model.fast_generate(\n",
    "    [text],\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request= model.load_lora(path_lora),\n",
    ")[0].outputs[0].text\n",
    "\n",
    "print(f\"Problem:\\n{train_dataset[idx][\"question\"]}\")\n",
    "print(f\"Response:\\n{output}\")\n",
    "print(\"GT Answer:\", train_dataset[idx][\"answer\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We could expand the method for more complicated problems or integrate with other diverse evaluating signals to further optimize the reasoning chians of LLMs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import create_repo\n",
    "\n",
    "repo_id = \"Savoxism/grpo-lora-vietnam-llm\"\n",
    "create_repo(repo_id, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, upload_folder\n",
    "\n",
    "api = HfApi()\n",
    "upload_folder(\n",
    "    folder_path=\"grpo_saved_lora\",  \n",
    "    repo_id=repo_id,                \n",
    "    repo_type=\"model\"               \n",
    ")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "model = PeftModel.from_pretrained(\"Savoxism/grpo-lora-vietnam-llm\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
